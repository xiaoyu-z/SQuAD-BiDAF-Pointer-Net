{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import numpy as np\n",
    "\n",
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER\n",
    "\n",
    "\n",
    "def get_initializer(matrix):\n",
    "    def _initializer(shape, dtype=None, partition_info=None, **kwargs): return matrix\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      initializer: initializer for Variable\n",
    "\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "    Note that the Variable is initialized with a truncated normal distribution.\n",
    "    A weight decay is added only if one is specified.\n",
    "\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      stddev: standard deviation of a truncated Gaussian\n",
    "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "          decay is not added for this Variable.\n",
    "\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    var = variable_on_cpu(name, shape,\n",
    "                           tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if wd:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "\n",
    "    Args:\n",
    "      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "        is over individual gradients. The inner list is over the gradient\n",
    "        calculation for each tower.\n",
    "    Returns:\n",
    "       List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "       across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, var in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            assert g is not None, var.name\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "\n",
    "def mask(val, mask, name=None):\n",
    "    if name is None:\n",
    "        name = 'mask'\n",
    "    return tf.multiply(val, tf.cast(mask, 'float'), name=name)\n",
    "\n",
    "\n",
    "def exp_mask(val, mask, name=None):\n",
    "    \"\"\"Give very negative number to unmasked elements in val.\n",
    "    For example, [-3, -2, 10], [True, True, False] -> [-3, -2, -1e9].\n",
    "    Typically, this effectively masks in exponential space (e.g. softmax)\n",
    "    Args:\n",
    "        val: values to be masked\n",
    "        mask: masking boolean tensor, same shape as tensor\n",
    "        name: name for output tensor\n",
    "\n",
    "    Returns:\n",
    "        Same shape as val, where some elements are very small (exponentially zero)\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"exp_mask\"\n",
    "    return tf.add(val, (1 - tf.cast(mask, 'float')) * VERY_NEGATIVE_NUMBER, name=name)\n",
    "\n",
    "\n",
    "def flatten(tensor, keep):\n",
    "    fixed_shape = tensor.get_shape().as_list()\n",
    "    start = len(fixed_shape) - keep\n",
    "    left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)])\n",
    "    out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))]\n",
    "    flat = tf.reshape(tensor, out_shape)\n",
    "    return flat\n",
    "\n",
    "\n",
    "def reconstruct(tensor, ref, keep):\n",
    "    ref_shape = ref.get_shape().as_list()\n",
    "    tensor_shape = tensor.get_shape().as_list()\n",
    "    ref_stop = len(ref_shape) - keep\n",
    "    tensor_start = len(tensor_shape) - keep\n",
    "    pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)]\n",
    "    keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))]\n",
    "    # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]\n",
    "    # keep_shape = tensor.get_shape().as_list()[-keep:]\n",
    "    target_shape = pre_shape + keep_shape\n",
    "    out = tf.reshape(tensor, target_shape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_wd(wd, scope=None):\n",
    "    scope = scope or tf.get_variable_scope().name\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "    with tf.name_scope(\"weight_decay\"):\n",
    "        for var in variables:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\"{}/wd\".format(var.op.name))\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None, shorten=False, num_groups=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue, ) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = (tuple(e for e in each if e is not None) for each in out)\n",
    "    return out\n",
    "\n",
    "def padded_reshape(tensor, shape, mode='CONSTANT', name=None):\n",
    "    paddings = [[0, shape[i] - tf.shape(tensor)[i]] for i in range(len(shape))]\n",
    "    return tf.pad(tensor, paddings, mode=mode, name=name)\n",
    "\n",
    "\n",
    "def get_num_params():\n",
    "    num_params = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        num_params += reduce(mul, [dim.value for dim in shape], 1)\n",
    "    return num_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell_impl import _linear\n",
    "from tensorflow.python.util import nest\n",
    "import tensorflow as tf\n",
    "\n",
    "from my.tensorflow import flatten, reconstruct, add_wd, exp_mask\n",
    "\n",
    "\n",
    "def linear(args, output_size, bias, bias_start=0.0, scope=None, squeeze=False, wd=0.0, input_keep_prob=1.0,\n",
    "           is_train=None):\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    flat_args = [flatten(arg, 1) for arg in args]\n",
    "    if input_keep_prob < 1.0:\n",
    "        assert is_train is not None\n",
    "        flat_args = [tf.cond(is_train, lambda: tf.nn.dropout(arg, input_keep_prob), lambda: arg)\n",
    "                     for arg in flat_args]\n",
    "    with tf.variable_scope(scope or 'Linear'):\n",
    "        flat_out = _linear(flat_args, output_size, bias, bias_initializer=tf.constant_initializer(bias_start))\n",
    "    out = reconstruct(flat_out, args[0], 1)\n",
    "    if squeeze:\n",
    "        out = tf.squeeze(out, [len(args[0].get_shape().as_list())-1])\n",
    "    if wd:\n",
    "        add_wd(wd)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob, is_train, noise_shape=None, seed=None, name=None):\n",
    "    with tf.name_scope(name or \"dropout\"):\n",
    "        if keep_prob < 1.0:\n",
    "            d = tf.nn.dropout(x, keep_prob, noise_shape=noise_shape, seed=seed)\n",
    "            out = tf.cond(is_train, lambda: d, lambda: x)\n",
    "            return out\n",
    "        return x\n",
    "\n",
    "\n",
    "def softmax(logits, mask=None, scope=None):\n",
    "    with tf.name_scope(scope or \"Softmax\"):\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        flat_logits = flatten(logits, 1)\n",
    "        flat_out = tf.nn.softmax(flat_logits)\n",
    "        out = reconstruct(flat_out, logits, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def softsel(target, logits, mask=None, scope=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param target: [ ..., J, d] dtype=float\n",
    "    :param logits: [ ..., J], dtype=float\n",
    "    :param mask: [ ..., J], dtype=bool\n",
    "    :param scope:\n",
    "    :return: [..., d], dtype=float\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope or \"Softsel\"):\n",
    "        a = softmax(logits, mask=mask)\n",
    "        target_rank = len(target.get_shape().as_list())\n",
    "        out = tf.reduce_sum(tf.expand_dims(a, -1) * target, target_rank - 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "def double_linear_logits(args, size, bias, bias_start=0.0, scope=None, mask=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"Double_Linear_Logits\"):\n",
    "        first = tf.tanh(linear(args, size, bias, bias_start=bias_start, scope='first',\n",
    "                               wd=wd, input_keep_prob=input_keep_prob, is_train=is_train))\n",
    "        second = linear(first, 1, bias, bias_start=bias_start, squeeze=True, scope='second',\n",
    "                        wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        if mask is not None:\n",
    "            second = exp_mask(second, mask)\n",
    "        return second\n",
    "\n",
    "\n",
    "def linear_logits(args, bias, bias_start=0.0, scope=None, mask=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"Linear_Logits\"):\n",
    "        logits = linear(args, 1, bias, bias_start=bias_start, squeeze=True, scope='first',\n",
    "                        wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def sum_logits(args, mask=None, name=None):\n",
    "    with tf.name_scope(name or \"sum_logits\"):\n",
    "        if args is None or (nest.is_sequence(args) and not args):\n",
    "            raise ValueError(\"`args` must be specified\")\n",
    "        if not nest.is_sequence(args):\n",
    "            args = [args]\n",
    "        rank = len(args[0].get_shape())\n",
    "        logits = sum(tf.reduce_sum(arg, rank-1) for arg in args)\n",
    "        if mask is not None:\n",
    "            logits = exp_mask(logits, mask)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_logits(args, size, bias, bias_start=0.0, scope=None, mask=None, wd=0.0, input_keep_prob=1.0, is_train=None, func=None):\n",
    "    if func is None:\n",
    "        func = \"sum\"\n",
    "    if func == 'sum':\n",
    "        return sum_logits(args, mask=mask, name=scope)\n",
    "    elif func == 'linear':\n",
    "        return linear_logits(args, bias, bias_start=bias_start, scope=scope, mask=mask, wd=wd, input_keep_prob=input_keep_prob,\n",
    "                             is_train=is_train)\n",
    "    elif func == 'double':\n",
    "        return double_linear_logits(args, size, bias, bias_start=bias_start, scope=scope, mask=mask, wd=wd, input_keep_prob=input_keep_prob,\n",
    "                                    is_train=is_train)\n",
    "    elif func == 'dot':\n",
    "        assert len(args) == 2\n",
    "        arg = args[0] * args[1]\n",
    "        return sum_logits([arg], mask=mask, name=scope)\n",
    "    elif func == 'mul_linear':\n",
    "        assert len(args) == 2\n",
    "        arg = args[0] * args[1]\n",
    "        return linear_logits([arg], bias, bias_start=bias_start, scope=scope, mask=mask, wd=wd, input_keep_prob=input_keep_prob,\n",
    "                             is_train=is_train)\n",
    "    elif func == 'proj':\n",
    "        assert len(args) == 2\n",
    "        d = args[1].get_shape()[-1]\n",
    "        proj = linear([args[0]], d, False, bias_start=bias_start, scope=scope, wd=wd, input_keep_prob=input_keep_prob,\n",
    "                      is_train=is_train)\n",
    "        return sum_logits([proj * args[1]], mask=mask)\n",
    "    elif func == 'tri_linear':\n",
    "        assert len(args) == 2\n",
    "        new_arg = args[0] * args[1]\n",
    "        return linear_logits([args[0], args[1], new_arg], bias, bias_start=bias_start, scope=scope, mask=mask, wd=wd, input_keep_prob=input_keep_prob,\n",
    "                             is_train=is_train)\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "def highway_layer(arg, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"highway_layer\"):\n",
    "        d = arg.get_shape()[-1]\n",
    "        trans = linear([arg], d, bias, bias_start=bias_start, scope='trans', wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        trans = tf.nn.relu(trans)\n",
    "        gate = linear([arg], d, bias, bias_start=bias_start, scope='gate', wd=wd, input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "        gate = tf.nn.sigmoid(gate)\n",
    "        out = gate * trans + (1 - gate) * arg\n",
    "        return out\n",
    "\n",
    "\n",
    "def highway_network(arg, num_layers, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0, is_train=None):\n",
    "    with tf.variable_scope(scope or \"highway_network\"):\n",
    "        prev = arg\n",
    "        cur = None\n",
    "        for layer_idx in range(num_layers):\n",
    "            cur = highway_layer(prev, bias, bias_start=bias_start, scope=\"layer_{}\".format(layer_idx), wd=wd,\n",
    "                                input_keep_prob=input_keep_prob, is_train=is_train)\n",
    "            prev = cur\n",
    "        return cur\n",
    "\n",
    "\n",
    "def conv1d(in_, filter_size, height, padding, is_train=None, keep_prob=1.0, scope=None):\n",
    "    with tf.variable_scope(scope or \"conv1d\"):\n",
    "        num_channels = in_.get_shape()[-1]\n",
    "        filter_ = tf.get_variable(\"filter\", shape=[1, height, num_channels, filter_size], dtype='float')\n",
    "        bias = tf.get_variable(\"bias\", shape=[filter_size], dtype='float')\n",
    "        strides = [1, 1, 1, 1]\n",
    "        if is_train is not None and keep_prob < 1.0:\n",
    "            in_ = dropout(in_, keep_prob, is_train)\n",
    "        xxc = tf.nn.conv2d(in_, filter_, strides, padding) + bias  # [N*M, JX, W/filter_stride, d]\n",
    "        out = tf.reduce_max(tf.nn.relu(xxc), 2)  # [-1, JX, d]\n",
    "        return out\n",
    "\n",
    "\n",
    "def multi_conv1d(in_, filter_sizes, heights, padding, is_train=None, keep_prob=1.0, scope=None):\n",
    "    with tf.variable_scope(scope or \"multi_conv1d\"):\n",
    "        assert len(filter_sizes) == len(heights)\n",
    "        outs = []\n",
    "        for filter_size, height in zip(filter_sizes, heights):\n",
    "            if filter_size == 0:\n",
    "                continue\n",
    "            out = conv1d(in_, filter_size, height, padding, is_train=is_train, keep_prob=keep_prob, scope=\"conv1d_{}\".format(height))\n",
    "            outs.append(out)\n",
    "        concat_out = tf.concat(axis=2, values=outs)\n",
    "        return concat_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn import dynamic_rnn as _dynamic_rnn, \\\n",
    "    bidirectional_dynamic_rnn as _bidirectional_dynamic_rnn\n",
    "\n",
    "from my.tensorflow import flatten, reconstruct\n",
    "\n",
    "\n",
    "def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None,\n",
    "                dtype=None, parallel_iterations=None, swap_memory=False,\n",
    "                time_major=False, scope=None):\n",
    "    assert not time_major  # TODO : to be implemented later!\n",
    "    flat_inputs = flatten(inputs, 2)  # [-1, J, d]\n",
    "    flat_len = None if sequence_length is None else tf.cast(flatten(sequence_length, 0), 'int64')\n",
    "\n",
    "    flat_outputs, final_state = _dynamic_rnn(cell, flat_inputs, sequence_length=flat_len,\n",
    "                                             initial_state=initial_state, dtype=dtype,\n",
    "                                             parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n",
    "                                             time_major=time_major, scope=scope)\n",
    "\n",
    "    outputs = reconstruct(flat_outputs, inputs, 2)\n",
    "    return outputs, final_state\n",
    "\n",
    "\n",
    "def bw_dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None,\n",
    "                   dtype=None, parallel_iterations=None, swap_memory=False,\n",
    "                   time_major=False, scope=None):\n",
    "    assert not time_major  # TODO : to be implemented later!\n",
    "\n",
    "    flat_inputs = flatten(inputs, 2)  # [-1, J, d]\n",
    "    flat_len = None if sequence_length is None else tf.cast(flatten(sequence_length, 0), 'int64')\n",
    "\n",
    "    flat_inputs = tf.reverse(flat_inputs, 1) if sequence_length is None \\\n",
    "        else tf.reverse_sequence(flat_inputs, sequence_length, 1)\n",
    "    flat_outputs, final_state = _dynamic_rnn(cell, flat_inputs, sequence_length=flat_len,\n",
    "                                             initial_state=initial_state, dtype=dtype,\n",
    "                                             parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n",
    "                                             time_major=time_major, scope=scope)\n",
    "    flat_outputs = tf.reverse(flat_outputs, 1) if sequence_length is None \\\n",
    "        else tf.reverse_sequence(flat_outputs, sequence_length, 1)\n",
    "\n",
    "    outputs = reconstruct(flat_outputs, inputs, 2)\n",
    "    return outputs, final_state\n",
    "\n",
    "\n",
    "def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n",
    "                              initial_state_fw=None, initial_state_bw=None,\n",
    "                              dtype=None, parallel_iterations=None,\n",
    "                              swap_memory=False, time_major=False, scope=None):\n",
    "    assert not time_major\n",
    "\n",
    "    flat_inputs = flatten(inputs, 2)  # [-1, J, d]\n",
    "    flat_len = None if sequence_length is None else tf.cast(flatten(sequence_length, 0), 'int64')\n",
    "\n",
    "    (flat_fw_outputs, flat_bw_outputs), final_state = \\\n",
    "        _bidirectional_dynamic_rnn(cell_fw, cell_bw, flat_inputs, sequence_length=flat_len,\n",
    "                                   initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw,\n",
    "                                   dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n",
    "                                   time_major=time_major, scope=scope)\n",
    "\n",
    "    fw_outputs = reconstruct(flat_fw_outputs, inputs, 2)\n",
    "    bw_outputs = reconstruct(flat_bw_outputs, inputs, 2)\n",
    "    # FIXME : final state is not reshaped!\n",
    "    return (fw_outputs, bw_outputs), final_state\n",
    "\n",
    "\n",
    "def bidirectional_rnn(cell_fw, cell_bw, inputs,\n",
    "                      initial_state_fw=None, initial_state_bw=None,\n",
    "                      dtype=None, sequence_length=None, scope=None):\n",
    "\n",
    "    flat_inputs = flatten(inputs, 2)  # [-1, J, d]\n",
    "    flat_len = None if sequence_length is None else tf.cast(flatten(sequence_length, 0), 'int64')\n",
    "\n",
    "    (flat_fw_outputs, flat_bw_outputs), final_state = \\\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, flat_inputs, sequence_length=flat_len,\n",
    "                                        initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw,\n",
    "                                        dtype=dtype, scope=scope)\n",
    "\n",
    "    fw_outputs = reconstruct(flat_fw_outputs, inputs, 2)\n",
    "    bw_outputs = reconstruct(flat_bw_outputs, inputs, 2)\n",
    "    # FIXME : final state is not reshaped!\n",
    "    return (fw_outputs, bw_outputs), final_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
