{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "#from process_data import save_pickle, load_pickle, load_task, load_processed_json, load_glove_weights\n",
    "#from process_data import to_var, to_np, make_vector\n",
    "#from process_data import DataSet\n",
    "from BiDAF_PointerNet import BiDAF_PrNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]\n",
      "                             [--lr_rate LR_RATE] [--evaluate EVALUATE]\n",
      "                             [--n_epoch N_EPOCH] [--resume_file PATH]\n",
      "                             [--pointer POINTER]\n",
      "                             [--cross_entropy CROSS_ENTROPY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/zhengxiaoyu/Library/Jupyter/runtime/kernel-b48b9111-03fa-45f4-b05c-299fb39a58ad.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengxiaoyu/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=24)\n",
    "parser.add_argument('--lr_rate', type=float, default=0.05)\n",
    "parser.add_argument('--evaluate', type=bool, default=False)\n",
    "parser.add_argument('--n_epoch', type=int, default = 20)\n",
    "parser.add_argument('--resume_file', type=str, default='./save/',metavar='PATH')\n",
    "parser.add_argument('--pointer', type=bool, default=True)\n",
    "parser.add_argument('--cross_entropy', type=bool, default=False)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-529a5f9e7d77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mydata():\n",
    "    def __init__(self, dataset1, dataset2, batch_size):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.length = len(self.dataset1['q']) \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def get_batch(self, current):\n",
    "        batches = []\n",
    "        batch = []\n",
    "        for i in range(self.batch_size): \n",
    "        \n",
    "            context  = self.dataset2['x'][self.dataset1['*x'][current+i][0]][self.dataset1['*x'][current+i][1]][0]\n",
    "            query  = self.dataset1['q'][current+i]\n",
    "            context = [j.lower() for j in context]\n",
    "            query = [j.lower() for j in query]\n",
    "            ans  = (self.dataset1['y'][i][0][0][1], self.dataset1['y'][i][0][1][1]) \n",
    "            batch.append((context, query, ans))\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "                \n",
    "    def get_word_index(self, word_num=5):\n",
    "\n",
    "        word2vec_dict = self.dataset2['lower_word2vec']\n",
    "        word_counter = self.dataset2['lower_word_counter']\n",
    "        char_counter = self.dataset2['char_counter']\n",
    "        windex = {w: i for i, w in enumerate(w for w, ct in word_counter.items()\n",
    "                                            if ct > word_num or (w in word2vec_dict))}\n",
    "        #c2i = {c: i for i, c in\n",
    "        #            enumerate(c for c, ct in char_counter.items()\n",
    "        #                      if ct > char_count_th)}\n",
    "        # w2i[NULL] = 0\n",
    "        # w2i[UNK] = 1\n",
    "        # w2i[ENT] = 2\n",
    "        # c2i[NULL] = 0\n",
    "        # c2i[UNK] = 1\n",
    "        # c2i[ENT] = 2\n",
    "        return windex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args):\n",
    "        self.read_data()\n",
    "        args.vocab_size_w = len(self.word2index)\n",
    "        self.args = args\n",
    "        self.model = BiDAF_PrNet(self.args)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.current = 0\n",
    "        self.start_epoch = 0\n",
    "            \n",
    "    def load_dev_data(self, path='./dataset/'):\n",
    "        test_json = json.load(open(path+'data_dev.json'))\n",
    "        test_shared_json = json.load(open(path+'shared_dev.json'))\n",
    "        self.testdata = MyData(test_json, test_shared_json, self.args.batch_size)\n",
    "    \n",
    "    def load_train_data(self, path='./dataset/'):\n",
    "        test_json = json.load(open(path+'data_train.json'))\n",
    "        test_shared_json = json.load(open(path+'shared_train.json'))\n",
    "        self.traindata = MyData(test_json, test_shared_json, self.args.batch_size)\n",
    "    \n",
    "    def read_data(self):\n",
    "        #if self.args.evaluate:\n",
    "        #train_json, train_shared_json = load_processed_json('./dataset/data_train.json', './dataset/shared_train.json')\n",
    "        #test_json, test_shared_json = load_processed_json('./dataset/data_test.json', './dataset/shared_test.json')\n",
    "        #self.train_data = DataSet(train_json, train_shared_json)\n",
    "        #self.test_data = DataSet(test_json, test_shared_json)\n",
    "        #ctx_maxlen = train_data.get_ctx_maxlen()\n",
    "        #ctx_sent_maxlen, query_sent_maxlen = train_data.get_sent_maxlen()\n",
    "        # ctx_word_maxlen, query_word_maxlen = train_data.get_word_maxlen()\n",
    "        #w2i_train, c2i_train = train_data.get_word_index()\n",
    "        self.load_train_data()\n",
    "        self.load_dev_data()\n",
    "        \n",
    "        word_index_train = self.traindata.get_word_index()\n",
    "        word_index_test = self.testdata.get_word_index()\n",
    "        #w2i_test, c2i_test = test_data.get_word_index()\n",
    "        word_vocabulary = sorted(list(set(list(word_index_train.keys()) + list(word_index_test.keys()))))\n",
    "        self.word2index = {w : i for i, w in enumerate(word_vocabulary, 3)}\n",
    "        #vocabs_c = sorted(list(set(list(c2i_train.keys()) + list(c2i_test.keys()))))\n",
    "        #c2i = {c : i for i, c in enumerate(vocabs_c, 3)}\n",
    "        self.word2index[\"-NULL-\"] = 0\n",
    "        self.word2index[\"-UNK-\"] = 1\n",
    "        self.word2index[\"-ENT-\"] = 2\n",
    "    \n",
    "    def self.tovariable(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "        \n",
    "    def to_vector(self, batch, context_sent_len, query_sent_len):\n",
    "        c = []\n",
    "        q = []\n",
    "        ans = []\n",
    "        # c, cc, q, cq, a in batch\n",
    "        for d in batch:\n",
    "            c.append(_make_word_vector(d[0], self.word2index, context_sent_len))\n",
    "            #cc.append(_make_char_vector(d[1], c2i, ctx_sent_len, ctx_word_len))\n",
    "            q.append(_make_word_vector(d[2], self.word2index, query_sent_len))\n",
    "            #cq.append(_make_char_vector(d[3], c2i, query_sent_len, query_word_len))\n",
    "            ans.append(d[-1])\n",
    "        c = self.to_variable(torch.LongTensor(c))\n",
    "        #cc = self.to_variable(torch.stack(cc, 0))\n",
    "        q = self.to_variable(torch.LongTensor(q))\n",
    "        #cq = to_var(torch.stack(cq, 0))\n",
    "        a = self.to_variable(torch.LongTensor(ans))\n",
    "        return c, q, a\n",
    "\n",
    "    def load_model(self, path):\n",
    "        return\n",
    "    \n",
    "    def train(self, n_epoch=self.args.n_epoch, optimizer, start_epoch=0, batch_size=self.args.batch_size):\n",
    "        self.model.train()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
